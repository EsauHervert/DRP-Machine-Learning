{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression f(x) = w0 + w1x",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EsauHervert/DRP-Machine-Learning/blob/master/Linear_Regression_f(x)_%3D_w0_%2B_w1x.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxfbKpg6NfnY",
        "colab_type": "text"
      },
      "source": [
        "We have done the case where $f(x) = wx$, now let us look at the case where $f(x) = w^0 + w^1x$. Define the vector $w = (w^0, w^1)$, we can define the following:\n",
        "\n",
        "Given an arbitrary $w = (w^0,w^1)$, the loss is defined as:\n",
        "\n",
        "$$\\mathcal{L}(X,Y,w) = \\frac{1}{2n}\\sum_{i=1}^n\\left(w^0 + w^1x_i - y_i)\\right)^2$$\n",
        "\n",
        "Its gradient is the following:\n",
        "\n",
        "$$\\nabla_w\\mathcal{L}(X,Y,w) = \\left(\\frac{\\partial}{\\partial w_0}\\mathcal{L}(X,Y,w), \\frac{\\partial}{\\partial w_1}\\mathcal{L}(X,Y,w)\\right)$$\n",
        "\n",
        "where\n",
        "\n",
        "\\begin{align*}\n",
        "  \\frac{\\partial}{\\partial w_0}\\mathcal{L}(X,Y,w) &= \\frac{\\partial}{\\partial w_0}\\frac{1}{2n}\\sum_{i=1}^n\\left(w^0 + w^1x_i - y_i)\\right)^2 \\\\\n",
        "  &= \\frac{1}{n}\\sum_{i=1}^n(w^0 + w^1x_i - y_i)\\frac{\\partial}{\\partial w_0}(w^0 + w^1x_i - y_i)\\\\\n",
        "  &= \\frac{1}{n}\\sum_{i=1}^n(w^0 + w^1x_i - y_i)\n",
        "\\end{align*}\n",
        "\n",
        "\\begin{align*}\n",
        "  \\frac{\\partial}{\\partial w_1}\\mathcal{L}(X,Y,w) &= \\frac{\\partial}{\\partial w_1}\\frac{1}{2n}\\sum_{i=1}^n\\left(w^0 + w^1x_i - y_i)\\right)^2 \\\\\n",
        "  &= \\frac{1}{n}\\sum_{i=1}^n(w^0 + w^1x_i - y_i)\\frac{\\partial}{\\partial w_1}(w^0 + w^1x_i - y_i)\\\\\n",
        "  &= \\frac{1}{n}\\sum_{i=1}^n(w^0 + w^1x_i - y_i)*x_i\n",
        "\\end{align*}\n",
        "\n",
        "$$\\therefore \n",
        "So now our gradient update rule:\n",
        "\n",
        "$$w_{j+1} = w_{j} - \\eta \\nabla_w\\mathcal{L}(X,Y,w_j)$$\n",
        "\n",
        "defining the following:\n",
        "\n",
        "$$\\tilde{x}_i = \\begin{bmatrix}\n",
        "1\\\\\n",
        "x_i\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "We can define the following:\n",
        "\n",
        "$$w^t\\tilde{x}_i = w_0+w_1x_i$$\n",
        "\n",
        "$$\\nabla_w\\mathcal{L}(X,Y,w) = \\frac{1}{n}\\sum_{i=1}^n(w^T\\tilde{x}_i - y_i)*\\tilde{x}_i := \\delta(w)$$\n",
        "\n",
        "Thus the update rule becomes:\n",
        "\n",
        "$$w_{j+1} = w_j - \\eta\\delta(w_j)$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLB2Zg0SNMnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKecZWa_R6t5",
        "colab_type": "text"
      },
      "source": [
        "Our target function will be the function $f(x) = 2 - 3x$ on the interval $[-1,1]$. We will first do everything in terms of vanilla gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0c3UzZiR4zN",
        "colab_type": "code",
        "outputId": "0246dbe7-217e-4f8c-8d35-3435fdcbaa11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X = torch.arange(-1,1,0.01)\n",
        "n = X.numpy().size\n",
        "X = torch.reshape(X, (1, n))\n",
        "Y = 2 - 3*X\n",
        "print(Y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8glrR-POSa1v",
        "colab_type": "text"
      },
      "source": [
        "Now since we are given a vector $X = [x_1 x_2 \\cdots x_n]$ and we need to instead have a matrix $\\tilde{X} = [\\tilde{x}_1 \\tilde{x}_2\\cdots\\tilde{x}_n]$, we do the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuLbEw4WTdKB",
        "colab_type": "code",
        "outputId": "8e74d041-b57a-462a-8a85-8d5f24b1230e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "Ones = torch.ones(1,n)\n",
        "Xt = torch.cat((Ones, X), 0)\n",
        "print(Xt.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iigBLkzTU3VI",
        "colab_type": "text"
      },
      "source": [
        "To simplify our calculations, we will define the following:\n",
        "\n",
        "$$e_i = w^T\\tilde{x}_i - y_i$$\n",
        "\n",
        "$$E = w^T\\tilde{X} - Y$$\n",
        "\n",
        "$$\\therefore \\delta(w) =  \\frac{1}{n}\\sum_{i=1}^ne_i*\\tilde{x}_i  = (E\\circ\\tilde{X}).mean(1)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLant7AFTngg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GradDescent(X, Y, w, eta, N, t):\n",
        "  t_0 = time.time()\n",
        "  n,m = X.size()\n",
        "  for j in range(N):\n",
        "    wT = torch.transpose(w, 0, 1)\n",
        "    E = torch.mm(wT, X) - Y\n",
        "    if E.norm(p=2) <= t:\n",
        "      break\n",
        "    delta = (E*X).mean(1)\n",
        "    delta = torch.reshape(delta, (n, 1))\n",
        "    w -= eta*delta\n",
        "  t_1 = time.time()\n",
        "  print(\"total time = \",t_1 - t_0)\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRRMPbV-j0uR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sGradDescent(X, Y, w, eta, N, s, t):\n",
        "  t_0 = time.time()\n",
        "  n,m = X.size()\n",
        "  for j in range(N):\n",
        "    perm = torch.randperm(m)\n",
        "    perm = perm[0:s]\n",
        "    Xpi = X[:,perm]\n",
        "    #print(Xpi.size())\n",
        "    Ypi = Y[0,perm]\n",
        "    #print(Ypi.size())\n",
        "    wT = torch.transpose(w, 0, 1)\n",
        "    #print(wT)\n",
        "    Epi = torch.mm(wT, Xpi) - Ypi\n",
        "    if Epi.norm(p=2) <= t:\n",
        "      break\n",
        "    delta = (Epi*Xpi).mean(1)\n",
        "    delta = torch.reshape(delta, (n,1))\n",
        "    w -= eta*delta\n",
        "  t_1 = time.time()\n",
        "  print(\"total time = \",t_1-t_0)\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA_jVoOMXeLx",
        "colab_type": "text"
      },
      "source": [
        "Now let us initialize a $w$ and see how it performs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ31XEAEUtGI",
        "colab_type": "code",
        "outputId": "5fab7b87-af73-4585-8fd6-3557b484158a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "w = torch.rand(2,1)\n",
        "w_new = w.clone()\n",
        "w_new_s = w.clone()\n",
        "print(w)\n",
        "print(Y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.4340],\n",
            "        [0.6091]])\n",
            "torch.Size([1, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYNxMyFwjUcc",
        "colab_type": "text"
      },
      "source": [
        "We will define a function that will take in the inputs and a predeterminded weights and get an output based on the weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HhsTV9aXjg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(w,x):\n",
        "  wT = torch.transpose(w, 0, 1)\n",
        "  return torch.mm(wT, x)\n",
        "def loss(w,x,y):\n",
        "  return f(w,x) - y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ac75PyeXxe-",
        "colab_type": "code",
        "outputId": "b59b5f33-5095-4a66-e14d-392260d7ca69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "fig, (ax1) = plt.subplots(1, 1)\n",
        "ax1.plot(X, f(w, Xt), marker = '_', color = 'blue')\n",
        "ax1.scatter(X, Y, marker='x', color='red')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f64f508a7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuf8HNAcC2K/wENMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVnZ2isXhFtY",
        "colab_type": "text"
      },
      "source": [
        "Now let us run our gradient descent to see what kind of value we get.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIVooBpXX30I",
        "colab_type": "code",
        "outputId": "8f6095ed-65d5-455d-f3e5-48fdcad7aa97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "w_new = GradDescent(Xt, Y, w_new, 0.01, 1000, 0.01)\n",
        "w_new_s = sGradDescent(Xt, Y, w_new_s, 0.01, 1000, 50, 0.01)\n",
        "print(w_new)\n",
        "print(w_new_s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total time =  0.09580349922180176\n",
            "total time =  0.13677334785461426\n",
            "tensor([[ 2.0009],\n",
            "        [-2.8724]])\n",
            "tensor([[ 2.0017],\n",
            "        [-2.8735]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhwRoX5BhPF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, (ax1) = plt.subplots(1, 1)\n",
        "ax1.plot(X, f(w_new, Xt), marker = '_', color = 'blue')\n",
        "ax1.scatter(X, Y, marker='x', color='red')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z18BVMGFjAjo",
        "colab_type": "code",
        "outputId": "c13cfc7f-0b6f-45dd-d4c8-081acc52444e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "fig, (ax1) = plt.subplots(1, 1)\n",
        "ax1.plot(X, loss(w_new, Xt), marker = '_', color = 'blue')\n",
        "ax1.plot(X, loss(w_new_s), marker='x', color='red')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-2559b608f154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_new_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: loss() missing 1 required positional argument: 'y'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAANT0lEQVR4nO3cYYjkd33H8ffHO1NpjKb0VpC706T0\n0njYQtIlTRFqirZc8uDugUXuIFgleGAbKVWEFEuU+MiGWhCu1ZOKVdAYfSALntwDjQTEC7chNXgX\nItvTeheFrDHNk6Ax7bcPZtKdrneZf3Zndy/7fb/gYP7/+e3Mlx97752d2ZlUFZKk7e8VWz2AJGlz\nGHxJasLgS1ITBl+SmjD4ktSEwZekJqYGP8lnkzyZ5PuXuD5JPplkKcmjSW6c/ZiSpPUa8gj/c8CB\nF7n+VmDf+N9R4F/WP5YkadamBr+qHgR+/iJLDgGfr5FTwNVJXj+rASVJs7FzBrexGzg/cXxhfO6n\nqxcmOcrotwCuvPLKP7z++utncPeS1MfDDz/8s6qaW8vXziL4g1XVceA4wPz8fC0uLm7m3UvSy16S\n/1zr187ir3SeAPZOHO8Zn5MkXUZmEfwF4F3jv9a5GXimqn7t6RxJ0taa+pROki8BtwC7klwAPgK8\nEqCqPgWcAG4DloBngfds1LCSpLWbGvyqOjLl+gL+emYTSZI2hO+0laQmDL4kNWHwJakJgy9JTRh8\nSWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+\nJDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZf\nkpow+JLUhMGXpCYMviQ1YfAlqYlBwU9yIMnjSZaS3HWR69+Q5IEkjyR5NMltsx9VkrQeU4OfZAdw\nDLgV2A8cSbJ/1bK/B+6vqhuAw8A/z3pQSdL6DHmEfxOwVFXnquo54D7g0Ko1BbxmfPm1wE9mN6Ik\naRaGBH83cH7i+ML43KSPArcnuQCcAN5/sRtKcjTJYpLF5eXlNYwrSVqrWb1oewT4XFXtAW4DvpDk\n1267qo5X1XxVzc/Nzc3oriVJQwwJ/hPA3onjPeNzk+4A7geoqu8CrwJ2zWJASdJsDAn+aWBfkmuT\nXMHoRdmFVWt+DLwNIMmbGAXf52wk6TIyNfhV9TxwJ3ASeIzRX+OcSXJPkoPjZR8E3pvke8CXgHdX\nVW3U0JKkl27nkEVVdYLRi7GT5+6euHwWeMtsR5MkzZLvtJWkJgy+JDVh8CWpCYMvSU0YfElqwuBL\nUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAl\nqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS\n1ITBl6QmDL4kNTEo+EkOJHk8yVKSuy6x5p1JziY5k+SLsx1TkrReO6ctSLIDOAb8GXABOJ1koarO\nTqzZB/wd8JaqejrJ6zZqYEnS2gx5hH8TsFRV56rqOeA+4NCqNe8FjlXV0wBV9eRsx5QkrdeQ4O8G\nzk8cXxifm3QdcF2S7yQ5leTAxW4oydEki0kWl5eX1zaxJGlNZvWi7U5gH3ALcAT4TJKrVy+qquNV\nNV9V83NzczO6a0nSEEOC/wSwd+J4z/jcpAvAQlX9qqp+CPyA0Q8ASdJlYkjwTwP7klyb5ArgMLCw\nas3XGD26J8kuRk/xnJvhnJKkdZoa/Kp6HrgTOAk8BtxfVWeS3JPk4HjZSeCpJGeBB4APVdVTGzW0\nJOmlS1VtyR3Pz8/X4uLilty3JL1cJXm4qubX8rW+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0Y\nfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYM\nviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMG\nX5KaMPiS1ITBl6QmBgU/yYEkjydZSnLXi6x7R5JKMj+7ESVJszA1+El2AMeAW4H9wJEk+y+y7irg\nb4CHZj2kJGn9hjzCvwlYqqpzVfUccB9w6CLrPgZ8HPjFDOeTJM3IkODvBs5PHF8Yn/s/SW4E9lbV\n11/shpIcTbKYZHF5efklDytJWrt1v2ib5BXAJ4APTltbVcerar6q5ufm5tZ715Kkl2BI8J8A9k4c\n7xmfe8FVwJuBbyf5EXAzsOALt5J0eRkS/NPAviTXJrkCOAwsvHBlVT1TVbuq6pqqugY4BRysqsUN\nmViStCZTg19VzwN3AieBx4D7q+pMknuSHNzoASVJs7FzyKKqOgGcWHXu7kusvWX9Y0mSZs132kpS\nEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWp\nCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLU\nhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmhgU/CQHkjyeZCnJXRe5/gNJziZ5NMk3k7xx\n9qNKktZjavCT7ACOAbcC+4EjSfavWvYIMF9VfwB8FfiHWQ8qSVqfIY/wbwKWqupcVT0H3AccmlxQ\nVQ9U1bPjw1PAntmOKUlaryHB3w2cnzi+MD53KXcA37jYFUmOJllMsri8vDx8SknSus30RdsktwPz\nwL0Xu76qjlfVfFXNz83NzfKuJUlT7Byw5glg78TxnvG5/yfJ24EPA2+tql/OZjxJ0qwMeYR/GtiX\n5NokVwCHgYXJBUluAD4NHKyqJ2c/piRpvaYGv6qeB+4ETgKPAfdX1Zkk9yQ5OF52L/Bq4CtJ/j3J\nwiVuTpK0RYY8pUNVnQBOrDp398Tlt894LknSjPlOW0lqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHw\nJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4\nktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8\nSWrC4EtSEwZfkpoYFPwkB5I8nmQpyV0Xuf43knx5fP1DSa6Z9aCSpPWZGvwkO4BjwK3AfuBIkv2r\nlt0BPF1Vvwv8E/DxWQ8qSVqfIY/wbwKWqupcVT0H3AccWrXmEPBv48tfBd6WJLMbU5K0XjsHrNkN\nnJ84vgD80aXWVNXzSZ4Bfhv42eSiJEeBo+PDXyb5/lqG3oZ2sWqvGnMvVrgXK9yLFb+31i8cEvyZ\nqarjwHGAJItVNb+Z93+5ci9WuBcr3IsV7sWKJItr/dohT+k8AeydON4zPnfRNUl2Aq8FnlrrUJKk\n2RsS/NPAviTXJrkCOAwsrFqzAPzl+PJfAN+qqprdmJKk9Zr6lM74Ofk7gZPADuCzVXUmyT3AYlUt\nAP8KfCHJEvBzRj8Upjm+jrm3G/dihXuxwr1Y4V6sWPNexAfiktSD77SVpCYMviQ1seHB92MZVgzY\niw8kOZvk0STfTPLGrZhzM0zbi4l170hSSbbtn+QN2Ysk7xx/b5xJ8sXNnnGzDPg/8oYkDyR5ZPz/\n5LatmHOjJflskicv9V6ljHxyvE+PJrlx0A1X1Yb9Y/Qi738AvwNcAXwP2L9qzV8BnxpfPgx8eSNn\n2qp/A/fiT4HfHF9+X+e9GK+7CngQOAXMb/XcW/h9sQ94BPit8fHrtnruLdyL48D7xpf3Az/a6rk3\naC/+BLgR+P4lrr8N+AYQ4GbgoSG3u9GP8P1YhhVT96KqHqiqZ8eHpxi952E7GvJ9AfAxRp/L9IvN\nHG6TDdmL9wLHquppgKp6cpNn3CxD9qKA14wvvxb4ySbOt2mq6kFGf/F4KYeAz9fIKeDqJK+fdrsb\nHfyLfSzD7kutqarngRc+lmG7GbIXk+5g9BN8O5q6F+NfUfdW1dc3c7AtMOT74jrguiTfSXIqyYFN\nm25zDdmLjwK3J7kAnADevzmjXXZeak+ATf5oBQ2T5HZgHnjrVs+yFZK8AvgE8O4tHuVysZPR0zq3\nMPqt78Ekv19V/7WlU22NI8Dnquofk/wxo/f/vLmq/merB3s52OhH+H4sw4ohe0GStwMfBg5W1S83\nabbNNm0vrgLeDHw7yY8YPUe5sE1fuB3yfXEBWKiqX1XVD4EfMPoBsN0M2Ys7gPsBquq7wKsYfbBa\nN4N6stpGB9+PZVgxdS+S3AB8mlHst+vztDBlL6rqmaraVVXXVNU1jF7POFhVa/7QqMvYkP8jX2P0\n6J4kuxg9xXNuM4fcJEP24sfA2wCSvIlR8Jc3dcrLwwLwrvFf69wMPFNVP532RRv6lE5t3McyvOwM\n3It7gVcDXxm/bv3jqjq4ZUNvkIF70cLAvTgJ/HmSs8B/Ax+qqm33W/DAvfgg8Jkkf8voBdx3b8cH\niEm+xOiH/K7x6xUfAV4JUFWfYvT6xW3AEvAs8J5Bt7sN90qSdBG+01aSmjD4ktSEwZekJgy+JDVh\n8CWpCYMvSU0YfElq4n8BzPZcum6w2goAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQzOu9NGt9Hz",
        "colab_type": "code",
        "outputId": "b8747dfe-7518-4e5e-84e2-2af046c2651c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(abs(-5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}