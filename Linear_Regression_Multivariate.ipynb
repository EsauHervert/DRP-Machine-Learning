{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression Multivariate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPY04HtUJtQ9DrLoXHhpQ3H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EsauHervert/DRP-Machine-Learning/blob/master/Linear_Regression_Multivariate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1liBIATMAFA",
        "colab_type": "text"
      },
      "source": [
        "Let us look at the following problem:\n",
        "\n",
        "$$x = \\begin{bmatrix}\n",
        "x^1\\\\\n",
        "x^2\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$D = \\begin{bmatrix}\n",
        "D_1\\\\\n",
        "D_2\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$f(x) = D_1x^1+D_2x^2 = D^Tx$$\n",
        "\n",
        "Now, given some training data ${X,Y}$, how can we reconstruct $f$? We can do it by finding some $w$ such that $w\\approx D$. We can use the following to measure how close $w$ is to $D$:\n",
        "\n",
        "$$\\mathcal{L}(X,Y,w) = \\frac{1}{2n}\\sum_{i=1}^n(w^Tx_i - y_i)^2$$\n",
        "\n",
        "Using the gradient descent algorithm, we get:\n",
        "\n",
        "$$\\nabla_w\\mathcal{L}(X,Y,w) = \\frac{1}{n}\\sum_{i=1}^n(w^Tx_i-y_i)*x_i = \\delta(w)$$\n",
        "\n",
        "$$w_{j+1} = w_j - \\eta \\delta(w_j)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhB71oamk_zx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uovSaefAO7NF",
        "colab_type": "text"
      },
      "source": [
        "Now, let us look at the following function:\n",
        "\n",
        "$$f(x) = 2x^1 - 3x^2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPQqqqgeUBMB",
        "colab_type": "text"
      },
      "source": [
        "Note that for these algorithms to converge, we require that the data be features be independent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3JoiUC9L_jV",
        "colab_type": "code",
        "outputId": "1102b6f7-580e-4a10-f1d5-babf1933bf50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X1 = torch.rand(1,200)\n",
        "X2 = torch.rand(1,200)\n",
        "Y = 2*X1 - 3*X2\n",
        "print(Y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3AhxkU0L_mw",
        "colab_type": "code",
        "outputId": "0725767f-b278-4364-ecc5-4a89fc02216c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Xt = torch.cat((X1, X2), 0)\n",
        "print(Xt.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHD1tjLcPkRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GradDescent(X, Y, w, eta, N):\n",
        "  n,m = X.size()\n",
        "  for j in range(N):\n",
        "    wT = torch.transpose(w, 0, 1)\n",
        "    E = torch.mm(wT, X) - Y\n",
        "    delta = (E*X).mean(1)\n",
        "    delta = torch.reshape(delta, (n, 1))\n",
        "    w -= eta*delta\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JmLYhytPnBI",
        "colab_type": "code",
        "outputId": "001d1df3-978d-41ef-c8f2-5f1fa8c315a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "w = torch.rand(2,1)\n",
        "w_new = w.clone()\n",
        "print(w)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.5491],\n",
            "        [0.2267]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci4EUBbXQQY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(w,x):\n",
        "  wT = torch.transpose(w, 0, 1)\n",
        "  return torch.mm(wT, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA6NkDOAS4fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(X,Y,w):\n",
        "  E = f(w,X) - Y\n",
        "  return .5*(E*E).mean().item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc-dTycPTFLX",
        "colab_type": "code",
        "outputId": "138047e1-0fc7-4738-c1a9-13f1126d6e38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss(Xt,Y,w)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8937805891036987"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSqT5VnUTOg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GradDescent(X, Y, w, eta, N):\n",
        "  n,m = X.size()\n",
        "  for j in range(N):\n",
        "    E = f(w,X) - Y\n",
        "    delta = (E*X).mean(1)\n",
        "    delta = torch.reshape(delta, (n, 1))\n",
        "    w -= eta*delta\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_2p1u42TY31",
        "colab_type": "code",
        "outputId": "07fcb603-0e21-4002-af0d-90118b476949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "w_new = GradDescent(Xt, Y, w_new, .01, 1000)\n",
        "print(w_new)\n",
        "print(loss(Xt,Y, w_new))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.0001],\n",
            "        [-1.9619]])\n",
            "0.08688648045063019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "974CGF5fTwYV",
        "colab_type": "code",
        "outputId": "cc42dcd3-6d65-47a3-a5f3-5a5cb0434963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "w_new = GradDescent(Xt, Y, w_new, .01, 10000)\n",
        "print(w_new)\n",
        "print(loss(Xt,Y, w_new))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.9997],\n",
            "        [-2.9997]])\n",
            "5.8201861108386765e-09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pcUtVEsVb6a",
        "colab_type": "text"
      },
      "source": [
        "Now let us look at the function\n",
        "\n",
        "$$f(x) = x^1 - 5x^2 + 3x^3 +4x^4$$\n",
        "\n",
        "and see if our method works for this function as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyiLeuwhVp-6",
        "colab_type": "code",
        "outputId": "209b579c-26d3-465f-ed69-946f1748a625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "X1 = torch.rand(1,200)\n",
        "X2 = torch.rand(1,200)\n",
        "X3 = torch.rand(1,200)\n",
        "X4 = torch.rand(1,200)\n",
        "Y = X1 - 5*X2 + 3*X3 + 4*X4\n",
        "print(Y.size())\n",
        "\n",
        "Xt = torch.cat((X1, X2, X3, X4), 0)\n",
        "print(Xt.size())\n",
        "\n",
        "w = torch.rand(4,1)\n",
        "w_new = w.clone()\n",
        "print(w)\n",
        "\n",
        "loss(Xt,Y,w)\n",
        "\n",
        "w_new = GradDescent(Xt, Y, w_new, .01, 10000)\n",
        "print(w_new)\n",
        "print(loss(Xt,Y, w_new))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 200])\n",
            "torch.Size([4, 200])\n",
            "tensor([[0.2767],\n",
            "        [0.3214],\n",
            "        [0.0051],\n",
            "        [0.2987]])\n",
            "tensor([[ 0.9983],\n",
            "        [-4.9962],\n",
            "        [ 3.0012],\n",
            "        [ 3.9968]])\n",
            "9.916702765622176e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAWjuOlDWJdI",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the linear case, we can look at the quadratic case:\n",
        "\n",
        "$$f(x) = y_0 + D^Tx + x^TAx$$\n",
        "\n",
        "$$D = \\begin{bmatrix}\n",
        "d_1\\\\\n",
        "d_2\\\\\n",
        "\\vdots\\\\\n",
        "d_k\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$A = \\begin{bmatrix}\n",
        "a_{11} & a_{12} & \\cdots & a_{1k}\\\\\n",
        "a_{21} & a_{22} & \\cdots & a_{2k}\\\\\n",
        "\\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
        "a_{k1} & a_{k2} & \\cdots & a_{1k}\\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$f(x) = y_0 + d_1x^1 + d_2x^2+\\cdots + d_kx^k + a_{11}x^1x^1 + a_{12}x^1x^2 + \\cdots a_{21}x^2x^1 + \\cdots + a_{kk}x^kx^k$$\n",
        "\n",
        "We will assume that $A$ is symmetric, i.e. $a_{ij} = a_{ji}$. given that $x^ix^j = x^jx^i$, we get:\n",
        "\n",
        "$$f(x) = d_1x^1 + d_2x^2+\\cdots + d_kx^k + a_{11}x^1x^1 + 2a_{12}x^1x^2 + \\cdots +a_{ii}x^ix^i + 2a_ia_jx^ix^j + \\cdots + a_{kk}x^kx^k$$\n",
        "\n",
        "Like before we will look for a \"$w$\" that will minimize the MSE error of the data with our guess, The difference is that this time we will have a $w_0$, $D_w$,s and a $A_w$, i.e.\n",
        "\n",
        "$$f_w(x) = w_0 + D_w^Tx + x^TA_wx$$\n",
        "\n",
        "Defining a parameter $w$ to contain all the information about $w_0$, $D_w$, and $A_w$:\n",
        "\n",
        "$$w = \\begin{bmatrix}\n",
        "w_0\\\\\n",
        "D_w\\\\\n",
        "A_w\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Now, the MSE loss has the form:\n",
        "\n",
        "$$\\mathcal{L}(X,Y,w) = \\frac{1}{2n}\\sum_{i=1}^n \\left(w_0 + D_w^Tx_i + x_i^TA_wx_i -y_i\\right)^2$$\n",
        "\n",
        "We can define the gradient w.r.t. $w$ as:\n",
        "\n",
        "$$\\nabla_w = \\begin{bmatrix}\n",
        "\\nabla_{w_0}\\\\\n",
        "\\nabla_{D_w}\\\\\n",
        "\\nabla_{A_w}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Now note the following:\n",
        "\n",
        "$$\\nabla_w w = 1$$\n",
        "$$\\nabla_w w^Tx = x$$\n",
        "$$\\nabla_W x^TWx = xx^T$$\n",
        "\n",
        "So we have that using the above and the fact that \n",
        "\n",
        "\\begin{align*}\n",
        "  \\nabla_w \\mathcal{L}(X,Y,w) &= \\begin{bmatrix}\n",
        "\\nabla_{w_0}\\mathcal{L}(X,Y,w)\\\\\n",
        "\\nabla_{D_w}\\mathcal{L}(X,Y,w)\\\\\n",
        "\\nabla_{A_w}\\mathcal{L}(X,Y,w)\n",
        "\\end{bmatrix}\\\\\\\\\n",
        "  &=  \\begin{bmatrix}\n",
        "\\frac{1}{n}\\sum_{i=1}^n\\left(w_0 + D_w^Tx_i + x_i^TA_wx_i - y_i\\right)\\\\\n",
        "\\frac{1}{n}\\sum_{i=1}^n\\left(w_0 + D_w^Tx_i + x_i^TA_wx_i - y_i\\right)*x_i\\\\\n",
        "\\frac{1}{n}\\sum_{i=1}^n\\left(w_0 + D_w^Tx_i + x_i^TA_wx_i - y_i\\right)*x_ix_i^T\n",
        "\\end{bmatrix}\\\\\\\\\n",
        "  &= \\frac{1}{n}\\sum_{i=1}^n\\left(w_0 + D_w^Tx_i + x_i^TA_wx_i - y_i\\right)\\begin{bmatrix}\n",
        "1\\\\\n",
        "x_i\\\\\n",
        "x_ix_i^T\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "Defining the error as:\n",
        "$$e_i(w) = w_0 + D_w^Tx_i + x_i^TA_wx_i - y_i$$\n",
        "\n",
        "we can define two deltas:\n",
        "\n",
        "$$\\delta_0(w) = \\frac{1}{n}\\sum_{i=1}^ne_i(w)$$\n",
        "\n",
        "$$\\delta_1(w) = \\frac{1}{n}\\sum_{i=1}^ne_i(w)*x_i$$\n",
        "\n",
        "$$\\delta_2(w) = \\frac{1}{n}\\sum_{i=1}^ne_i(w)*x_ix_i^T$$\n",
        "\n",
        "Thus we can define our update rules as follows:\n",
        "\n",
        "$$w_o(j+1) = w_0(j) - \\eta \\delta_0(w_j)$$\n",
        "\n",
        "$$D_w(j+1) = D_w(j) - \\eta \\delta_1(w_j)$$\n",
        "\n",
        "$$A_w(j+1) = A_w(j) - \\eta \\delta_2(w_j)$$\n",
        "\n",
        "Note that since $xx^T$ is symmetric, we have that the update will return a symmetric matrix for $A_w$ as long as $A_w(1)$ is symmetrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZEMG5_RRNk9",
        "colab_type": "text"
      },
      "source": [
        "Now, let us look at the following problem on the interval $[-1,1]^2$:\n",
        "\n",
        "$$f(x) = 5 + 2x^1 + 3x^2 + x^1x^1 + 4x^1x^2 + x^2x^2$$\n",
        "\n",
        "$$y_0 = 5$$\n",
        "\n",
        "$$D = \\begin{bmatrix}\n",
        "2\\\\\n",
        "3\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$A = \\begin{bmatrix}\n",
        "1 & 2\\\\\n",
        "2 & 1\n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QjjfUC_SFkb",
        "colab_type": "code",
        "outputId": "40eadadd-88da-49c4-9a39-678c62eae933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X1 = 2*torch.rand(1,200) - 1\n",
        "X2 = 2*torch.rand(1,200) - 1\n",
        "Y = 5 + 2*X1 + 3*X2 + X1*X1 + 4*X1*X2 + X2*X2\n",
        "print(Y.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttAZejuBTnI9",
        "colab_type": "code",
        "outputId": "b50079e8-d556-4d58-e3b6-32b0e832970b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Xt = torch.cat((X1,X2), 0)\n",
        "print(Xt.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoJ9Ck535kHu",
        "colab_type": "text"
      },
      "source": [
        "We will define a diag function since we only care of the $x_i^TA_wX_i$ terms in the matrix $X^TAX$ matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKiXXvj6Zg_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def diag(A):\n",
        "  n,m = A.size()\n",
        "  diag = torch.zeros(1,n)\n",
        "\n",
        "  for i in range(n):\n",
        "    diag[0, i] = A[i,i]\n",
        "  \n",
        "  return diag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpAAwcwT5xIc",
        "colab_type": "text"
      },
      "source": [
        "Now defining the function that given some data $X = \\begin{bmatrix}\n",
        "x_1 \\cdots x_n\\end{bmatrix}$ and some weights $w_0$, $D_w$, and $A_w$, we get\n",
        "\n",
        "$$f(x;w_0, D_w, A_w) = w_0 + D_w^Tx + x^TA_wx$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbxBbgEES5iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(X, w0, Dw, Aw):\n",
        "  DwT = torch.transpose(Dw, 0, 1)\n",
        "  DwTX = torch.mm(DwT, X)\n",
        "  XT = torch.transpose(X, 0, 1)\n",
        "  XTAw = torch.mm(XT, Aw)\n",
        "  XTAwX = torch.mm(XTAw, X)\n",
        "  \n",
        "  return w0 + DwTX + diag(XTAwX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mwaLD_L6Vdf",
        "colab_type": "text"
      },
      "source": [
        "Now, we can generate random tensors, but we need a symmetric matrix so we need to create a function that will turn a random matrix to a symmetric matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvMZNwskfTBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def symm(A):\n",
        "  n,m = A.size()\n",
        "  for i in range(n):\n",
        "    for j in range(n - i - 1):\n",
        "      k = n - j - 1\n",
        "      A[i, k] = A[k, i]\n",
        "  \n",
        "  return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj-I1qU_Sj1x",
        "colab_type": "code",
        "outputId": "b1e5b8e4-e6a2-4067-fe07-dabe4e4dcf5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "w0 = torch.rand(1,1)\n",
        "Dw = torch.rand(2,1)\n",
        "Aw = torch.rand(2,2)\n",
        "Aw = symm(Aw)\n",
        "\n",
        "print(w0)\n",
        "print(Dw)\n",
        "print(Aw)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.8250]])\n",
            "tensor([[0.1653],\n",
            "        [0.5387]])\n",
            "tensor([[0.1283, 0.2407],\n",
            "        [0.2407, 0.7997]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDb2ZdfaT0h0",
        "colab_type": "code",
        "outputId": "87a38c13-e026-4a6c-a589-3866ab6dcf7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f(Xt, w0, Dw, Aw).size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN4x2TgJ7xhA",
        "colab_type": "text"
      },
      "source": [
        "Now, defining a loss function for all three of the weight sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlU8HjFET7Yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(X, Y, w0, Dw, Aw):\n",
        "  E = f(X, w0, Dw, Aw) - Y\n",
        "  return .5*(E*E).mean().item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3rX2dvDakpB",
        "colab_type": "code",
        "outputId": "3e895b64-d091-4ec6-f517-11f6c4ba5f25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss(Xt, Y, w0, Dw, Aw)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.208385467529297"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLd8BaZi77MO",
        "colab_type": "text"
      },
      "source": [
        "Now, we will use the class Weights so that we can return multiple set of weights in the gradient descent algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhdNQiYFeF48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Weights():\n",
        "    def __init__(self, scalar, vector, matrix):\n",
        "        self.scalar = scalar\n",
        "        self.vector = vector\n",
        "        self.matrix = matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kiXyWwD8Dan",
        "colab_type": "text"
      },
      "source": [
        "Now here is the gradient descent rule that we derived."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP_wwRTbaoK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GradDescent(X, Y, w0, Dw, Aw, eta, N):\n",
        "  m,n = X.size()\n",
        "\n",
        "  # This will hold all our weights\n",
        "  W = Weights(w0, Dw, Aw)\n",
        "\n",
        "  for j in range(N):\n",
        "\n",
        "    # We calculate the function values given the current weights\n",
        "    F = f(Xt, w0, Dw, Aw)\n",
        "\n",
        "    # This is the delta values that we will use to update the weights\n",
        "    delta0 = torch.zeros(1,1)\n",
        "    delta1 = torch.zeros(m,1)\n",
        "    delta2 = torch.zeros(m,m)\n",
        "\n",
        "    # To calculate the delta values we will loop through the samples for simplicity sake\n",
        "    for i in range(n):\n",
        "      xi = torch.reshape(Xt[:,i], (m, 1))\n",
        "      yi = Y[:,i]\n",
        "      fi = F[:,i]\n",
        "      ei = fi - yi\n",
        "\n",
        "      delta0 += ei\n",
        "      delta1 += ei*xi\n",
        "      delta2 += ei*torch.mm(xi, torch.transpose(xi, 0, 1))\n",
        "    \n",
        "    # Average\n",
        "    delta0 = (1/n)*delta0\n",
        "    delta1 = (1/n)*delta1\n",
        "    delta2 = (1/n)*delta2\n",
        "\n",
        "    # Delta update rule\n",
        "    W.scalar -= eta*delta0\n",
        "    W.vector -= eta*delta1\n",
        "    W.matrix -= eta*delta2\n",
        "\n",
        "\n",
        "  return W"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUqZyB948J-B",
        "colab_type": "text"
      },
      "source": [
        "Now, running the algorithm we can see that this algorithm does in fact converge close to the true solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWLmg96oi24V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Weights = GradDescent(Xt, Y, w0, Dw, Aw, .01, 5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yazxNz2cjTbJ",
        "colab_type": "code",
        "outputId": "190290ae-273d-49d4-a5e0-ebbec8016d13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "w0f = Weights.scalar\n",
        "Dwf = Weights.vector\n",
        "Awf = Weights.matrix\n",
        "\n",
        "print(w0f)\n",
        "print(Dwf)\n",
        "print(Awf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.9806]])\n",
            "tensor([[2.0013],\n",
            "        [3.0006]])\n",
            "tensor([[1.0202, 2.0005],\n",
            "        [2.0005, 1.0327]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaD2M72nlgCw",
        "colab_type": "code",
        "outputId": "598ba1f1-65af-4575-cada-39feff8195b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss(Xt, Y, w0f, Dwf, Awf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.125760410213843e-05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}